{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"<div class=\"alert alert-block alert-success\" style=\"font-size:30px\">\nüß¨ NESP: ThermoNet v2 üß¨\n</div>\n\n<div class=\"alert alert-block alert-danger\" style=\"text-align:center; font-size:20px;\">\n    ‚ù§Ô∏è Dont forget to ‚ñ≤upvote‚ñ≤ if you find this notebook usefull!  ‚ù§Ô∏è\n</div>\n\nThis notebook implements a set of incremental improvements on top of the [NESP: ThermoNet notebook](https://www.kaggle.com/code/vslaykovsky/nesp-thermonet). It is largely inspired by the work of Bian Li,Yucheng T. Yang,John A. Capra ,Mark B. Gerstein: [Predicting changes in protein thermodynamic stability upon point mutation with deep 3D convolutional neural networks](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008291)\nOfficial ThermoNet repository can be found here: https://github.com/gersteinlab/ThermoNet\n\n\n## Results\n\nHere is a comparison between ThermoNet and ThermoNet2 models. First column shows performance of a simple ensemble built on cross-validate folds. The second column adds best public notebooks to the ensemble. \n\n\n\n| Model | <div style=\"width:300px\">LB: Base model (10 folds)</div> | <div style=\"width:300px\">LB: 10 folds + public notebooks ensemble</div> |\n| :- | :- | :- |\n| ThermoNet2 | **0.494** | **0.582** |\n| [ThermoNet](https://www.kaggle.com/code/vslaykovsky/nesp-thermonet) | 0.4 | 0.483 |\n\n\n\n\n## Improvements in v2 over v1\n\nHere are main differences with the baseline ThermoNet model:\n* **Larger training set (3214 -> 14656)**. This model uses the output of \"[**NESP: 14656 Unique Mutations + Voxel Features**](https://www.kaggle.com/vslaykovsky/nesp-9936-unique-mutations-voxel-features)\" notebook wich is AFAIK the largest dataset so far.\n* The model was trained using only **destabilizing mutations** ($\\Delta \\Delta G < 0$). This improved performance by a few percentage points. It seems like NESP test dataset is generated from more or less random mutations of the wildtype protein. It usually means that the majority of such mutations are destabilizing. Note: in this notebook we use the following signs of targets for destabilizing mutations: $\\Delta \\Delta G < 0$ and/or $\\Delta T < 0$. In other literature you might find different notations. \n* The model was rewritten to **PyTorch**. \n* Capacity of the model was increased - added **more convolutional layers**. This improved performance, likely thanks to larger dataset.\n* **Auxilliary target $\\Delta T$**. The \"[NESP: 14656 Unique Mutations + Voxel Features](https://www.kaggle.com/vslaykovsky/nesp-9936-unique-mutations-voxel-features)\" dataset contains both $\\Delta \\Delta G$ and $\\Delta T$ targets. Targets are strongly correlated with each other. So it makes sense to use both targets to train the backbone of the network while keeping separate fully-connected heads to account for different distributions between targets. Specifically, the training loss used in this notebook is $\\mathcal L= (y_{\\Delta \\Delta G} - \\hat{y}_{\\Delta \\Delta G})^2 + C * (y_{\\Delta T} - \\hat{y}_{\\Delta T})^2$, where $C=0.01$ is a hyperparameter that defines contribution of the $\\Delta T$ loss.\n* **Improved training features**. In the baseline ThermoNet model samples are generated with a simple concatenation of 7 voxel features calculated for each of wildtype and mutant proteins. It results in `concat(7x16x16x16, 7x16x16x16) == 14x16x16x16` shape (16 voxels in every direction, 14 channels per voxel). In this notebook we hypothesise that targets depend on the shape of the difference between voxel features of wildtype and mutant residues and significant part of learned logic in CNN layers is dedicated to calculating this difference. So we simplify this task by providing **differential features** along with normal ones. E.g. `difference_features = features[7:] - features[..., :7]`\n* **Hyperparameter optimization** with Optuna and Wandb sweeps. Best version is stored to `BEST_PARAMS` constant.\n\n\n## How does ThermoNet v2 work?\n\nThermoNet v2 is a 3D CNN with features generated from 3D structures of wildtype and mutant proteins. This is done in 3 steps: \n\n1. Accurate 3D structures of mutant proteins are generated from the wildtype template. In the paper authors use Rosetta \"FastRelax\" protocol to generate mutant PDB files. You can find this logic in \"[NESP: 14656 Unique Mutations + Voxel Features](https://www.kaggle.com/vslaykovsky/nesp-9936-unique-mutations-voxel-features)\". I generated all structures on my laptop, because It's close to impossible to recreate such PDB files in Kaggle boxes:\n    * It takes a lot of time. I generated test PDBS in ~10 hours on my 16 core laptop. \n    * Rosetta comes with a license that prohibits its redistribution. So you'll have to download and acquire free academic license by yourself in order to use Rosetta.\n\n2. Now once we have all PDB structures, voxel features are generated from them. This is done with `Acellera htmd` library. Voxel features include a set of 7 attributes packed into a grid of size (16, 16, 16). 7 features names are: `'hydrophobic', 'aromatic', 'hbond_acceptor', 'hbond_donor', 'positive_ionizable', 'negative_ionizable', 'occupancies'`. For each training sample features come from both wildtype and mutant PDBs, so the final shape of each sample is: (14, 16, 16, 16)\n\n<img src=\"https://images2.imgbox.com/7c/5d/9mzuGf8N_o.png\" alt=\"image host\"/>\n\n3. Finally a simple VGG-style CNN is used to train the regression model. An ensemble of 10 models is used to predict the score. Only  $\\Delta \\Delta G$ output is used in prediction, $\\Delta T$ is only used in training as a part of the loss function. \n\n\n### Architecture of the model:\n\n<img src=\"https://images2.imgbox.com/1d/e9/UZ0tpNPy_o.png\" alt=\"image host\"/>\n\n\n## Additional notes\n* **[Didn't work]** Using $\\Delta T$ as the main target and $\\Delta \\Delta G$ as an auxilliary target. Perhaps there are too few $\\Delta T$ data in the training set. \n* The notebook is divided into sections. Each section can be enabled with it's own flag (TRAIN, OPTUNA, WANDB_SWEEP, SUBMISSION). All flags are defined in the \"Utils, imports\" section","metadata":{"papermill":{"duration":0.009889,"end_time":"2022-10-13T07:46:02.248412","exception":false,"start_time":"2022-10-13T07:46:02.238523","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils, imports","metadata":{"papermill":{"duration":0.006365,"end_time":"2022-10-13T07:46:02.261526","exception":false,"start_time":"2022-10-13T07:46:02.255161","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import multiprocessing\nimport os\n\nimport Levenshtein\nimport numpy as np\nfrom plotly.offline import init_notebook_mode\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\n\ninit_notebook_mode(connected=True)\nimport glob\nfrom scipy.stats import spearmanr\nfrom pprint import pprint\n\nimport plotly.express as px\nimport torch as th\nimport pandas as pd\nfrom scipy.stats import rankdata\nimport json\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom collections import defaultdict\nimport copy\nfrom torch.optim import AdamW\n\nMULTIPROCESSING = False\nBOXSIZE = 16\nVOXELSIZE = 1\nN_FOLDS = 10\nMODELS_PATH = 'models'\nDEBUG = True\nDEVICE = 'cuda' if th.cuda.is_available() else 'cpu'\nDESTABILIZING_MUTATIONS_ONLY = True\nAUGMENT_DESTABILIZING_MUTATIONS = False\nEARLY_STOPPING_PATIENCE = 30\nIS_DDG_TARGET = True\nWITH_PUCCI_SOURCE = True\nWITH_KAGGLE_DDG_SOURCE = True\n\n# switchers\nTRAIN = False\nWANDB_TRAIN_PROJECT = 'ThermoNetV2-train'\nWANDB_TRAIN_NAME = 'thermonetv2-7633-v2'\n\nOPTUNA = False\nOPTUNA_WANDB_PROJECT = \"ThermoNetV2-Optuna\"\nOPTUNA_TRIALS = 400\n\nWANDB_SWEEP = False\nWANDB_SWEEP_PROJECT = 'ThermoNetV2-sweep'\n\nSUBMISSION = True\n\n\nDEFAULT_PARAMS = {\n    'SiLU': False,\n    'diff_features': True,\n    'LayerNorm': False,\n    'GroupKFold': False,  # only use for hyperopt\n    'epochs': 30,\n    'AdamW': False,\n}\n\n\n\n\n\nBEST_PARAMS = {**DEFAULT_PARAMS, **{'AdamW': True,\n 'C_dt_loss': 0.01,\n 'OneCycleLR': False,\n 'batch_size': 256,\n 'AdamW_decay': 1.3994535042337082,\n 'dropout_rate': 0.06297340526648805,\n 'learning_rate': 0.00020503764745082723,\n 'conv_layer_num': 5,\n 'dropout_rate_dt': 0.3153179929570238,\n 'dense_layer_size': 74.1731281147114}}\n\n\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    WANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n    print('Running in Kaggle')\n    WILDTYPE_PDB = '../input/novozymes-enzyme-stability-prediction/wildtype_structure_prediction_af2.pdb'\n    PDB_PATH = '../input/thermonet-wildtype-relaxed'\n    TRAIN_FEATURES_PATH = '../input/thermonet-features/Q3214.npy'\n    TRAIN_TARGETS_PATH = ''\n    TEST_CSV = '../input/novozymes-enzyme-stability-prediction/test.csv'\n    TEST_FEATURES_PATH = '../input/thermonet-features/nesp_features.npy'\n    PUBLIC_SUBMISSIONS=[\n        '../input/rmsd-from-molecular-dynamics/submission_rmsd.csv',     # LB: 0.507\n        '../input/plldt-ddg-demask-sasa/deepddg-ddg.csv',                # LB: 0.451\n        '../input/novo-esp-eli5-performant-approaches-lb-0-451/submission.csv',  # 0.451\n        '../input/nesp-alphafold-getarea-exploration/submission.csv',                   # 0.407\n    ]    \n    TRAIN_FEATURES_DIR = '../input/nesp-9936-unique-mutations-voxel-features'\n    \n\nexcept Exception as ex:\n    print('Running locally')\n    WILDTYPE_PDB = 'nesp/thermonet/wildtypeA.pdb'\n    PDB_PATH = 'nesp/thermonet/'\n    TRAIN_FEATURES_PATH = 'data/train_features/features.npy'\n    TRAIN_TARGETS_PATH = 'data/train_features/dataset.csv'\n    TEST_FEATURES_PATH = 'data/nesp/nesp_features.npy'\n    TEST_CSV = 'data/nesp/test.csv'\n    PUBLIC_SUBMISSIONS=glob.glob('data/nesp/public_submissions/*.csv')\n    TRAIN_FEATURES_DIR = 'data/train_features/'\n    WANDB_API_KEY='your_key_here'\n\nos.makedirs(MODELS_PATH, exist_ok=True)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":8.766915,"end_time":"2022-10-13T07:47:46.023202","exception":false,"start_time":"2022-10-13T07:47:37.256287","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-31T07:54:24.800018Z","iopub.execute_input":"2022-10-31T07:54:24.800416Z","iopub.status.idle":"2022-10-31T07:54:25.085255Z","shell.execute_reply.started":"2022-10-31T07:54:24.800383Z","shell.execute_reply":"2022-10-31T07:54:25.084145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n\n\"\"\"\nAdd WANDB_API_KEY with your wandb.ai API key to run the code. \n\"\"\"\nwandb.login(key=WANDB_API_KEY)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-31T07:54:25.087687Z","iopub.execute_input":"2022-10-31T07:54:25.088404Z","iopub.status.idle":"2022-10-31T07:54:25.361781Z","shell.execute_reply.started":"2022-10-31T07:54:25.088363Z","shell.execute_reply":"2022-10-31T07:54:25.360631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load training data","metadata":{"papermill":{"duration":0.024167,"end_time":"2022-10-13T07:47:46.072494","exception":false,"start_time":"2022-10-13T07:47:46.048327","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def load_data():\n    print(\"1. Loading csv datasets\")\n    df = pd.read_csv(f'{TRAIN_FEATURES_DIR}/dataset.csv')\n    df.source = df.source.apply(eval)\n    print(f'Total unique mutations: {len(df)}')\n\n    df['features'] = df.apply(lambda r: f'{TRAIN_FEATURES_DIR}/features/{r.PDB_chain}_{r.wildtype}{r.pdb_position}{r.mutant}.npy', axis=1)\n    df = df[df.features.apply(lambda v: os.path.exists(v))]\n\n    print(f'Total mutations with features: {len(df)}')\n\n    if not WITH_PUCCI_SOURCE:\n        df = df[df.source.apply(lambda v: v != ['pucci-proteins-appendixtable1.xlsx'])]\n\n    if not WITH_KAGGLE_DDG_SOURCE:\n        df = df[df.source.apply(lambda v: v != ['ddg-xgboost-5000-mutations-200-pdb-files-lb-0-40.csv'])]\n\n    print(f'Total mutations after filtering: {len(df)}')\n\n    df.features = [np.load(f) for f in tqdm(df.features, desc=\"2. Loading features\")]\n\n\n    df_train = df\n\n    if DESTABILIZING_MUTATIONS_ONLY:\n        print('Keeping destabilizing mutations only')\n        df_train = df_train[((df_train.ddG < 0))  & ((df_train.dT < 0) | df_train.dT.isna())].reset_index(drop=True).copy() # best for ddG\n    elif AUGMENT_DESTABILIZING_MUTATIONS:\n        print('Augmenting destabilizinb mutations')\n        df_pos = df_train[df_train.ddG > 0].copy()\n        df_neg = df_train[df_train.ddG < 0]\n        print(df_pos.shape, df_neg.shape)\n        df_pos.features = df_pos.features.apply(lambda f: np.concatenate([f[7:], f[:7]], axis=0))\n        df_pos.ddG = -df_pos.ddG\n        df_pos.dT = -df_pos.dT\n        df_train = pd.concat([df_pos, df_neg], axis=0).sample(frac=1.).reset_index(drop=True)        \n    return df_train\n\ndf_train = load_data()\ndf_train","metadata":{"jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2022-10-31T07:54:25.363441Z","iopub.execute_input":"2022-10-31T07:54:25.364166Z","iopub.status.idle":"2022-10-31T07:56:20.576712Z","shell.execute_reply.started":"2022-10-31T07:54:25.364037Z","shell.execute_reply":"2022-10-31T07:56:20.574501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.dT.plot.hist(title='Distribution of dT')","metadata":{"jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2022-10-31T07:56:20.57956Z","iopub.execute_input":"2022-10-31T07:56:20.579936Z","iopub.status.idle":"2022-10-31T07:56:20.817903Z","shell.execute_reply.started":"2022-10-31T07:56:20.579899Z","shell.execute_reply":"2022-10-31T07:56:20.816957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.ddG.plot.hist(title='Distribution of ddG')","metadata":{"jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2022-10-31T07:56:20.821732Z","iopub.execute_input":"2022-10-31T07:56:20.822023Z","iopub.status.idle":"2022-10-31T07:56:21.039047Z","shell.execute_reply.started":"2022-10-31T07:56:20.821996Z","shell.execute_reply":"2022-10-31T07:56:21.038161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.plot.scatter(x='ddG', y='dT', title='ddG vs dT')","metadata":{"execution":{"iopub.status.busy":"2022-10-31T07:56:21.040356Z","iopub.execute_input":"2022-10-31T07:56:21.040811Z","iopub.status.idle":"2022-10-31T07:56:21.276182Z","shell.execute_reply.started":"2022-10-31T07:56:21.040774Z","shell.execute_reply":"2022-10-31T07:56:21.275211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.groupby('sequence').features.count().plot.hist(title='Mutations per sequence', bins=50)","metadata":{"execution":{"iopub.status.busy":"2022-10-31T07:56:21.277816Z","iopub.execute_input":"2022-10-31T07:56:21.278465Z","iopub.status.idle":"2022-10-31T07:56:21.550147Z","shell.execute_reply.started":"2022-10-31T07:56:21.278425Z","shell.execute_reply":"2022-10-31T07:56:21.549178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting voxel representation of features\n\nIn the following plots we use 3D scatterplot to demonstrate training samples. \nSpecifically we plot `occupancy` feature that represents probability that certain voxel is occpupied by an atom.\nRecall that each training/test sample uses a combination of wildetype+mutant features. So we use the following color-coding:\n* blue color represents voxels that are occupied in both wildtype and mutant structures \n* red color represents voxels that are occupied only in the mutant structure\n* green color represents voxels that are occupied only in the wildtype structure","metadata":{"papermill":{"duration":0.024859,"end_time":"2022-10-13T07:47:57.242588","exception":false,"start_time":"2022-10-13T07:47:57.217729","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\n\ndef plot_voxels():\n    for i in [123, 124, 125, 126]:\n        df = pd.DataFrame([(x, y1, z) for x in range(16) for y1 in range(16) for z in range(16)], columns=['x', 'y', 'z'])\n        df['occupancy1'] = df_train.iloc[i].features[6, :, :, :].flatten() > 0.9\n        df['occupancy2'] = df_train.iloc[i].features[13, :, :, :].flatten() > 0.9\n        df.loc[df.occupancy1 | df.occupancy2, 'color'] = 'blue'\n        df.loc[~df.occupancy1 & df.occupancy2, 'color'] = 'red'\n        df.loc[df.occupancy1 & ~df.occupancy2, 'color'] = 'green'\n        ddg = df_train.ddG[i]\n        fig = px.scatter_3d(df.dropna(), x='x', y='y', z='z', color='color', title=f\"Train idx:{i}; ddg={ddg}\")\n        fig.show()\n        \n\nplot_voxels()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":1.285929,"end_time":"2022-10-13T07:47:58.553486","exception":false,"start_time":"2022-10-13T07:47:57.267557","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-31T07:56:21.551506Z","iopub.execute_input":"2022-10-31T07:56:21.552165Z","iopub.status.idle":"2022-10-31T07:56:21.822532Z","shell.execute_reply.started":"2022-10-31T07:56:21.552126Z","shell.execute_reply":"2022-10-31T07:56:21.821661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class ThermoNet2(th.nn.Module):\n    def __init__(self, params):\n        super().__init__()\n\n        CONV_LAYER_SIZES = [14, 16, 24, 32, 48, 78, 128]\n        FLATTEN_SIZES = [0, 5488, 5184, 4000, 3072, 2106, 1024]\n\n        dropout_rate = params['dropout_rate']\n        dropout_rate_dt = params['dropout_rate_dt']\n        dense_layer_size = int(params['dense_layer_size'])\n        layer_num = int(params['conv_layer_num'])\n        silu = params['SiLU']\n\n        self.params = params\n        if silu:\n            activation = th.nn.SiLU()\n        else:\n            activation = th.nn.ReLU()\n\n        model = [\n            th.nn.Sequential(\n                *[th.nn.Sequential(\n                    th.nn.Conv3d(in_channels=CONV_LAYER_SIZES[l], out_channels=CONV_LAYER_SIZES[l + 1], kernel_size=(3, 3, 3)),\n                    activation\n                ) for l in range(layer_num)]\n            ),\n            th.nn.MaxPool3d(kernel_size=(2,2,2)),\n            th.nn.Flatten(),\n        ]\n        flatten_size = FLATTEN_SIZES[layer_num]\n        if self.params['LayerNorm']:\n            model.append(th.nn.LayerNorm(flatten_size))\n        self.model = th.nn.Sequential(*model)\n\n        self.ddG = th.nn.Sequential(\n            th.nn.Dropout(p=dropout_rate),\n            th.nn.Linear(in_features=flatten_size, out_features=dense_layer_size),\n            activation,\n            th.nn.Dropout(p=dropout_rate),\n            th.nn.Linear(in_features=dense_layer_size, out_features=1)\n        )\n        self.dT = th.nn.Sequential(\n            th.nn.Dropout(p=dropout_rate_dt),\n            th.nn.Linear(in_features=flatten_size, out_features=dense_layer_size),\n            activation,\n            th.nn.Dropout(p=dropout_rate_dt),\n            th.nn.Linear(in_features=dense_layer_size, out_features=1)\n        )\n\n\n    def forward(self, x):\n        if self.params['diff_features']:\n            x[:, 7:, ...] -= x[:, :7, ...]\n        x = self.model(x)\n        ddg = self.ddG(x)\n        dt = self.dT(x)\n        return ddg.squeeze(), dt.squeeze()\n\nif DEBUG:\n    params = copy.copy(BEST_PARAMS)\n    params['diff_features'] = False\n    tn2 =ThermoNet2(params)\n    print([out.shape for out in tn2.forward(th.randn((2, 14, 16, 16, 16)))])\n    print(tn2)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-31T07:56:21.824216Z","iopub.execute_input":"2022-10-31T07:56:21.824613Z","iopub.status.idle":"2022-10-31T07:56:21.883916Z","shell.execute_reply.started":"2022-10-31T07:56:21.824575Z","shell.execute_reply":"2022-10-31T07:56:21.882823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class ThermoNet2Dataset(Dataset):\n    def __init__(self, df=None, features=None):\n        self.df = df\n        self.features = features\n\n    def __getitem__(self, item):\n        if self.df is not None:\n            r = self.df.iloc[item]\n            if 'ddG' in self.df.columns:\n                return th.as_tensor(r.features, dtype=th.float), th.tensor(r.ddG, dtype=th.float), th.tensor(r.dT, dtype=th.float)\n            else:\n                return th.as_tensor(r.features, dtype=th.float)\n        else:\n            return th.as_tensor(self.features[item], dtype=th.float)\n\n    def __len__(self):\n        return len(self.df) if self.df is not None else len(self.features)\n\nif DEBUG:\n    ds = ThermoNet2Dataset(df_train)\n    feat, t1, t2 = next(iter(DataLoader(ds, batch_size=BEST_PARAMS['batch_size'])))\n    print(feat.shape, t1.shape, t2.shape)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-31T07:56:21.885808Z","iopub.execute_input":"2022-10-31T07:56:21.886197Z","iopub.status.idle":"2022-10-31T07:56:22.090517Z","shell.execute_reply.started":"2022-10-31T07:56:21.886158Z","shell.execute_reply":"2022-10-31T07:56:22.089319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def evaluate(model, dl_val, params):\n    criterion = th.nn.MSELoss()\n    model.eval()\n    losses = []\n    ddg_preds = []\n    dt_preds = []\n    ddg_losses = []\n    dt_losses = []\n    with th.no_grad():\n        for x, ddg, dt in tqdm(dl_val, desc='Eval', disable=True):\n            ddg_pred, dt_pred = model(x.to(DEVICE))\n            ddg_preds.append(ddg_pred.cpu().numpy())\n            dt_preds.append(dt_pred.cpu().numpy())\n            ddg = ddg.to(DEVICE)\n            dt = dt.to(DEVICE)\n            not_nan_ddg = ~th.isnan(ddg)\n            ddg_loss = criterion(ddg[not_nan_ddg], ddg_pred[not_nan_ddg])\n\n            not_nan_dt = ~th.isnan(dt)\n            dt_loss = criterion(dt[not_nan_dt], dt_pred[not_nan_dt])\n\n            loss = th.stack([ddg_loss, dt_loss * params['C_dt_loss']])\n            loss = loss[~th.isnan(loss)].sum()\n            if not np.isnan(loss.item()):\n                losses.append(loss.item())\n            if not np.isnan(ddg_loss.item()):\n                ddg_losses.append(ddg_loss.item())\n            if not np.isnan(dt_loss.item()):\n                dt_losses.append(dt_loss.item())\n\n    return np.mean(losses), np.mean(ddg_losses), np.mean(dt_losses), np.concatenate(ddg_preds), np.concatenate(dt_preds)\n\n\ndef load_pytorch_model(fname, params=BEST_PARAMS):\n    model = ThermoNet2(params)\n    model.load_state_dict(th.load(fname))\n    return model\n\n\ndef train_model(name, dl_train, dl_val, params, wandb_enabled=True, project='thermonetv2'):\n    model = ThermoNet2(params).to(DEVICE)\n\n    if params['AdamW']:\n        def get_optimizer_params(model, encoder_lr, weight_decay=0.0):\n            no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n            optimizer_parameters = [\n                {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n                 'lr': encoder_lr, 'weight_decay': weight_decay},\n                {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n                 'lr': encoder_lr, 'weight_decay': 0.0},\n            ]\n            return optimizer_parameters\n\n        optimizer_parameters = get_optimizer_params(model,\n                                                    encoder_lr=params['learning_rate'],\n                                                    weight_decay=params['AdamW_decay'])\n        optim = AdamW(optimizer_parameters, lr=params['learning_rate'])\n    else:\n        optim = th.optim.Adam(model.parameters(), lr=params['learning_rate'])\n\n    scheduler = None\n    if params['OneCycleLR']:\n        from torch.optim.lr_scheduler import OneCycleLR\n        scheduler = OneCycleLR(optim, max_lr=params['learning_rate'],\n                                                     steps_per_epoch=len(dl_train), epochs=params['epochs'],\n                                                     pct_start=0.)\n    criterion = th.nn.MSELoss()\n    best_model = None\n    min_epoch = -1\n    val_losses = defaultdict(lambda: [])\n\n    run = None\n    if wandb_enabled:\n        run = wandb.init(project=project, name=name, mode='online' if wandb_enabled else 'disabled')\n\n    with tqdm(range(params['epochs']), desc='Epoch') as prog:\n        min_loss = np.inf\n        for epoch in prog:\n            model.train()\n            for x, ddg, dt in tqdm(dl_train, desc='Train', disable=True):\n                ddg_pred, dt_pred = model(x.to(DEVICE))\n                ddg = ddg.to(DEVICE)\n                dt = dt.to(DEVICE)\n                loss = None\n                any_ddg = ~th.isnan(ddg)\n                if th.any(any_ddg):\n                    loss = criterion(ddg[any_ddg], ddg_pred[any_ddg])\n                any_dt = ~th.isnan(dt)\n                if th.any(any_dt):\n                    dt_loss = criterion(dt[any_dt], dt_pred[any_dt])\n                    if loss is None:\n                        loss = dt_loss * params['C_dt_loss']\n                    else:\n                        loss += dt_loss * params['C_dt_loss']\n                optim.zero_grad()\n                loss.backward()\n                optim.step()\n                if scheduler is not None:\n                    scheduler.step()\n\n            eval_loss, eval_ddg_loss, eval_dt_loss = evaluate(model, dl_val, params)[:3]\n            val_losses['loss'].append(eval_loss)\n            val_losses['ddg_loss'].append(eval_ddg_loss)\n            val_losses['dt_loss'].append(eval_dt_loss)\n            if run is not None:\n                run.log({'val_loss': eval_loss, 'val_ddg_loss': eval_ddg_loss, 'val_dt_loss': eval_dt_loss,\n                         'lr': scheduler.get_last_lr()[0] if scheduler is not None else params['learning_rate']})\n            if eval_loss < min_loss:\n                min_loss = eval_loss\n                min_epoch = epoch\n                best_model = copy.deepcopy(model)\n            prog.set_description(\n                f'Epoch: {epoch}; Val MSE:{eval_loss:.02f}; Min Val MSE:{min_loss:.02f}; ddg loss:{eval_ddg_loss:.02f}; dT loss:{eval_dt_loss:.02f}')\n            if epoch - min_epoch > EARLY_STOPPING_PATIENCE:\n                print('Early stopping')\n                break\n\n    if run is not None:\n        art = wandb.Artifact(\"thermonet2\", type=\"model\")\n        fname = f'{MODELS_PATH}/{name}.pt'\n        th.save(model.state_dict(), fname)\n        art.add_file(fname)\n        run.log_artifact(art)\n        run.finish()\n    return best_model, val_losses\n\n\ndef run_train(name, params, project='thermonetv2'):\n    os.makedirs(MODELS_PATH, exist_ok=True)\n    val_losses = []\n    thermonet_models = []\n    kfold = GroupKFold(N_FOLDS)\n    if params['GroupKFold']:\n        groups = df_train.sequence\n    else:\n        groups = range(len(df_train))\n    for fold, (train_idx, val_idx) in enumerate(\n            tqdm(kfold.split(df_train, groups=groups), total=N_FOLDS, desc=\"Folds\")):\n        exp_name = f'{name}-{fold}'\n        fname = f'{MODELS_PATH}/{exp_name}.pt'\n        ds_train = ThermoNet2Dataset(df_train.loc[train_idx])\n        ds_val = ThermoNet2Dataset(df_train.loc[val_idx])\n\n        batch_size = params['batch_size']\n        dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n        dl_val = DataLoader(ds_val, batch_size=64, pin_memory=True, drop_last=True)\n\n        model, losses = train_model(exp_name, dl_train, dl_val, params, wandb_enabled=True, project=project)\n        val_losses.append(losses)\n        thermonet_models.append(model)\n\n    d = pd.DataFrame([{k: np.min(v) for k, v in l.items()} for l in val_losses]).mean().to_dict()\n    with wandb.init(project=f'{project}-CV', name=name) as run:\n        run.log(d)\n    return thermonet_models, d\n\n\nif TRAIN:\n    params = copy.copy(BEST_PARAMS)\n    thermonet_models = run_train(WANDB_TRAIN_NAME, params, project=WANDB_TRAIN_PROJECT)[0]","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-31T07:56:22.093753Z","iopub.execute_input":"2022-10-31T07:56:22.094044Z","iopub.status.idle":"2022-10-31T07:56:22.126547Z","shell.execute_reply.started":"2022-10-31T07:56:22.094016Z","shell.execute_reply":"2022-10-31T07:56:22.125286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%wandb -h 800 vslaykovsky/ThermoNetV2-train","metadata":{"execution":{"iopub.status.busy":"2022-10-31T07:56:22.128077Z","iopub.execute_input":"2022-10-31T07:56:22.128481Z","iopub.status.idle":"2022-10-31T07:56:22.145555Z","shell.execute_reply.started":"2022-10-31T07:56:22.128445Z","shell.execute_reply":"2022-10-31T07:56:22.144417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna Hyperparameter Optimization","metadata":{}},{"cell_type":"code","source":"if OPTUNA:\n\n    import optuna\n    from optuna.integration.wandb import WeightsAndBiasesCallback\n\n    wandbc = WeightsAndBiasesCallback(wandb_kwargs={\"project\": OPTUNA_WANDB_PROJECT}, as_multirun=True)\n\n    @wandbc.track_in_wandb()\n    def objective(trial):\n        params = copy.copy(DEFAULT_PARAMS)\n        params['conv_layer_num'] = trial.suggest_int('conv_layer_num', 3, 6)\n        params['AdamW'] = trial.suggest_categorical('AdamW', [True, False])\n        if params['AdamW']:\n            params['AdamW_decay'] = trial.suggest_float('AdamW_decay', 0.001, 100, log=True)\n        params['dense_layer_size'] = trial.suggest_int('dense_layer_size', 16, 128, log=True)\n\n        params['dropout_rate'] = trial.suggest_float('dropout_rate', 0., 0.7, log=False)\n        params['dropout_rate_dt'] = trial.suggest_float('dropout_rate_dt', 0., 0.7, log=False)\n        params['learning_rate'] = trial.suggest_float('learning_rate', 5e-6, 1e-3, log=True)\n        params['batch_size'] = trial.suggest_categorical('batch_size', [32, 64, 128, 256, 512])\n        params['C_dt_loss'] = trial.suggest_categorical('C_dt_loss', [0, 0.01, 0.1, 1.])\n        params['GroupKFold'] = True # works best for hyperparameter optimization\n        params['OneCycleLR'] = trial.suggest_categorical('OneCycleLR', [True, False])\n\n        print('params', params)\n        # --------------- train --------------\n        kfold = GroupKFold(5)\n        for train_idx, val_idx in kfold.split(df_train, groups=df_train.sequence):\n            ds_train = ThermoNet2Dataset(df_train.loc[train_idx])\n            ds_val = ThermoNet2Dataset(df_train.loc[val_idx])\n            batch_size = params['batch_size']\n            dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n            dl_val = DataLoader(ds_val, batch_size=64, pin_memory=True, shuffle=True, drop_last=True)\n            _, losses = train_model(\"optuna\", dl_train, dl_val, params, wandb_enabled=False)\n            return np.min(losses['ddg_loss' if IS_DDG_TARGET else 'dt_loss'])\n\n\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=OPTUNA_TRIALS, callbacks=[wandbc])","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-31T07:56:22.147572Z","iopub.execute_input":"2022-10-31T07:56:22.148136Z","iopub.status.idle":"2022-10-31T07:56:22.161197Z","shell.execute_reply.started":"2022-10-31T07:56:22.148101Z","shell.execute_reply":"2022-10-31T07:56:22.160223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%wandb -h 1200 vslaykovsky/ThermoNetV2-Optuna-fireprotdb","metadata":{"execution":{"iopub.status.busy":"2022-10-31T07:56:22.165774Z","iopub.execute_input":"2022-10-31T07:56:22.166058Z","iopub.status.idle":"2022-10-31T07:56:22.17842Z","shell.execute_reply.started":"2022-10-31T07:56:22.166021Z","shell.execute_reply":"2022-10-31T07:56:22.177293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wandb Sweeps Hyperparameter Optimization\n\nWandb sweeps are executed in 2 steps:\n1. Create sweep configuration in your Wandb project. This is done with `wandb.sweep` call below.\n2. Once you got your sweep id, pass it to your agents. Agents run tests using configuration passed from Wandb servers using `wandb.agent` call.\n\nSee more info on Wandb sweeps here https://docs.wandb.ai/guides/sweeps","metadata":{}},{"cell_type":"code","source":"# Set your sweep_id below to start optimization\n#%env SWEEP_ID=xxxxx   ","metadata":{"execution":{"iopub.status.busy":"2022-10-31T07:56:22.179883Z","iopub.execute_input":"2022-10-31T07:56:22.180347Z","iopub.status.idle":"2022-10-31T07:56:22.185925Z","shell.execute_reply.started":"2022-10-31T07:56:22.18028Z","shell.execute_reply":"2022-10-31T07:56:22.184926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if WANDB_SWEEP:\n    import wandb\n    sweep_id = os.environ.get('SWEEP_ID')\n    print('wandb sweep ', sweep_id)\n\n    if sweep_id is None:\n        \"\"\"\n        First run. Generate sweep_id. \n        \"\"\"\n        sweep_id = wandb.sweep(sweep={\n            'method': 'bayes',\n            'name': 'thermonet2-sweep',\n            'metric': {'goal': 'minimize', 'name': 'val_ddg_loss'},\n            'parameters':\n                {\n                    'conv_layer_num': {'values': [3, 4, 5, 6]},\n                    'AdamW': {'values': [True, False]},\n                    'AdamW_decay': {'min': 0.01, 'max': 5, 'distribution': 'log_uniform_values'},\n                    'dense_layer_size': {'min': 16, 'max': 128, 'distribution': 'log_uniform_values'},\n                    'dropout_rate': {'min': 0., 'max': 0.8, 'distribution': 'uniform'},\n                    'dropout_rate_dt': {'min': 0., 'max': 0.8, 'distribution': 'uniform'},\n                    'learning_rate': {'min': 1e-5, 'max': 3e-3, 'distribution': 'log_uniform_values'},\n                    'batch_size': {'values': [64, 128, 256, 512]},\n                    'OneCycleLR': {'values': [True, False]},\n                    'C_dt_loss': {'values': [0, 0.003, 0.01, 0.03]},\n                }\n        }, project=WANDB_SWEEP_PROJECT)\n        print('Generated sweep id', sweep_id)\n    else:\n        \"\"\"\n        Agent run. Use sweep_id generated above. \n        \"\"\"\n        def wandb_callback():\n            with wandb.init() as run:\n                params = copy.copy(DEFAULT_PARAMS)\n                params['conv_layer_num'] = run.config.conv_layer_num\n                params['AdamW'] = run.config.AdamW\n                params['AdamW_decay'] = run.config.AdamW_decay\n                params['dense_layer_size'] = int(run.config.dense_layer_size)\n                params['dropout_rate'] = run.config.dropout_rate\n                params['dropout_rate_dt'] = run.config.dropout_rate_dt\n                params['learning_rate'] = run.config.learning_rate\n                params['batch_size'] = run.config.batch_size\n                params['OneCycleLR'] = run.config.OneCycleLR\n                params['C_dt_loss'] = run.config.C_dt_loss\n                params['GroupKFold'] = True  # only for hyperparameter optimization\n                print('params', params)\n\n                # --------------- train --------------\n                kfold = GroupKFold(5)\n                for train_idx, val_idx in kfold.split(df_train, groups=df_train.sequence):\n                    ds_train = ThermoNet2Dataset(df_train.loc[train_idx])\n                    ds_val = ThermoNet2Dataset(df_train.loc[val_idx])\n                    batch_size = params['batch_size']\n                    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n                    dl_val = DataLoader(ds_val, batch_size=512, pin_memory=True, shuffle=True)\n                    _, losses = train_model(0, dl_train, dl_val, params, wandb_enabled=False)\n                    for epoch in range(len(losses['ddg_loss'])):\n                        run.log({\n                            'epoch': epoch,\n                            'val_loss': losses['loss'][epoch],\n                            'val_ddg_loss': losses['ddg_loss'][epoch],\n                            'val_dt_loss': losses['dt_loss'][epoch],\n                        })\n                    break\n\n\n\n        # Start sweep job.\n        wandb.agent(sweep_id, project=WANDB_SWEEP_PROJECT, function=wandb_callback, count=100000)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-31T07:56:22.187548Z","iopub.execute_input":"2022-10-31T07:56:22.188407Z","iopub.status.idle":"2022-10-31T07:56:22.204662Z","shell.execute_reply.started":"2022-10-31T07:56:22.188372Z","shell.execute_reply":"2022-10-31T07:56:22.203677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%wandb -h 1200 vslaykovsky/ThermoNetV2-fireprot-sweep/sweeps/vzyvxo1a","metadata":{"execution":{"iopub.status.busy":"2022-10-31T07:56:22.206347Z","iopub.execute_input":"2022-10-31T07:56:22.206731Z","iopub.status.idle":"2022-10-31T07:56:22.502741Z","shell.execute_reply.started":"2022-10-31T07:56:22.206689Z","shell.execute_reply":"2022-10-31T07:56:22.501758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n\nAll models are stored in Wandb, so downloading models to the localhost here.","metadata":{"papermill":{"duration":1.372033,"end_time":"2022-10-13T08:12:37.302323","exception":false,"start_time":"2022-10-13T08:12:35.93029","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def collect_wandb_models(name):\n    runs = wandb.Api().runs(\n        path='vslaykovsky/ThermoNetV2-train',\n    )\n    with tqdm(runs, desc='Downloading artefacts') as prog:\n        for run in prog:\n            art = run.logged_artifacts()\n            if len(art) > 0:\n                if name in run.name:\n                    prog.set_description(run.name)\n                    art[0].download()\n\n\nif SUBMISSION:\n    collect_wandb_models(WANDB_TRAIN_NAME)","metadata":{"execution":{"iopub.status.busy":"2022-10-31T07:56:22.505052Z","iopub.execute_input":"2022-10-31T07:56:22.50589Z","iopub.status.idle":"2022-10-31T07:56:35.26781Z","shell.execute_reply.started":"2022-10-31T07:56:22.505848Z","shell.execute_reply":"2022-10-31T07:56:35.26682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_mutations(name, df,\n                  wild=\"VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQ\"\"RVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGT\"\"NAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKAL\"\"GSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\"):\n    result = []\n    for _, r in df.iterrows():\n        ops = Levenshtein.editops(wild, r.protein_sequence)\n        assert len(ops) <= 1\n        if len(ops) > 0 and ops[0][0] == 'replace':\n            idx = ops[0][1]\n            result.append([ops[0][0], idx + 1, wild[idx], r.protein_sequence[idx]])\n        elif len(ops) == 0:\n            result.append(['same', 0, '', ''])\n        elif ops[0][0] == 'insert':\n            assert False, \"Ups\"\n        elif ops[0][0] == 'delete':\n            idx = ops[0][1]\n            result.append(['delete', idx + 1, wild[idx], '-'])\n        else:\n            assert False, \"Ups\"\n\n    df = pd.concat([df, pd.DataFrame(data=result, columns=['op', 'idx', 'wild', 'mutant'])], axis=1)\n    df['mut'] = df[['wild', 'idx', 'mutant']].astype(str).apply(lambda v: ''.join(v), axis=1)\n    df['name'] = name\n    return df\n\nif SUBMISSION:\n    df_test = gen_mutations('wildtypeA', pd.read_csv(TEST_CSV))\n    display(df_test)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.273512,"end_time":"2022-10-13T07:47:46.370401","exception":false,"start_time":"2022-10-13T07:47:46.096889","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-31T07:56:35.270052Z","iopub.execute_input":"2022-10-31T07:56:35.270978Z","iopub.status.idle":"2022-10-31T07:56:35.481341Z","shell.execute_reply.started":"2022-10-31T07:56:35.270937Z","shell.execute_reply":"2022-10-31T07:56:35.480213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model:ThermoNet2, test_features):\n    with th.no_grad():\n        model.eval()\n        dl = DataLoader(ThermoNet2Dataset(features=test_features), batch_size=64)\n        if IS_DDG_TARGET:\n            return np.concatenate(\n                [model.forward(x)[0].numpy() for x in tqdm(dl, desc='ThermoNet2 ddg predict', disable=True)])\n        else:\n            return np.concatenate(\n                [model.forward(x)[1].numpy() for x in tqdm(dl, desc='ThermoNet2 dt predict', disable=True)])","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-10-31T07:56:35.482918Z","iopub.execute_input":"2022-10-31T07:56:35.483396Z","iopub.status.idle":"2022-10-31T07:56:35.490668Z","shell.execute_reply.started":"2022-10-31T07:56:35.48335Z","shell.execute_reply":"2022-10-31T07:56:35.489465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if SUBMISSION:        \n    thermonet_models = [load_pytorch_model(f) for f in tqdm(glob.glob(f'artifacts/*/{WANDB_TRAIN_NAME}*.pt'), desc=f'Loading models {WANDB_TRAIN_NAME}')]\n\n    test_features = np.load(TEST_FEATURES_PATH)\n    test_ddg = np.stack([predict(model, test_features) for model in tqdm(thermonet_models, desc='Fold prediction')])\n    test_ddg = np.mean(test_ddg, axis=0).flatten()\n\n    # replacement mutations\n    df_test.loc[df_test.op == 'replace', 'ddg'] = test_ddg\n    # deletion mutations\n    df_test.loc[df_test['op'] == \"delete\", 'ddg'] = df_test[df_test[\"op\"]==\"replace\"][\"ddg\"].quantile(q=0.25)\n    # no mutations\n    df_test.loc[df_test['op'] == \"same\", 'ddg'] = 0.  \n\n    df_test.rename(columns={'ddg': 'tm'})[['seq_id', 'tm']].to_csv('submission.csv', index=False)\n    !head submission.csv","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"lines_to_next_cell":2,"papermill":{"duration":1.362245,"end_time":"2022-10-13T08:12:46.700062","exception":false,"start_time":"2022-10-13T08:12:45.337817","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-10-31T07:56:35.492209Z","iopub.execute_input":"2022-10-31T07:56:35.493014Z","iopub.status.idle":"2022-10-31T07:57:53.323905Z","shell.execute_reply.started":"2022-10-31T07:56:35.492978Z","shell.execute_reply":"2022-10-31T07:57:53.322678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble\n\nEnsembling ThermoNetV2 with top public solutions","metadata":{"papermill":{"duration":1.304016,"end_time":"2022-10-13T08:13:37.227676","exception":false,"start_time":"2022-10-13T08:13:35.92366","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if SUBMISSION:\n    \n    def ranked(f):\n        return rankdata(pd.read_csv(f).tm)\n    \n    pred = 0.7 * ranked('../input/rmsd-from-molecular-dynamics/submission_rmsd.csv')+\\\n        0.3 * (ranked('../input/plldt-ddg-demask-sasa/deepddg-ddg.csv')+        \\\n        ranked('../input/novo-esp-eli5-performant-approaches-lb-0-451/submission.csv')+ \\\n        ranked('../input/nesp-alphafold-getarea-exploration/submission.csv') + \\\n        ranked('submission.csv'))\n    \n    \n    df = pd.read_csv('../input/novozymes-enzyme-stability-prediction/sample_submission.csv')\n    df.tm = pred\n    \n    \n    # equally weighted ensemble with https://www.kaggle.com/code/shlomoron/nesp-relaxed-rosetta-scores\n    df.tm = rankdata(df.tm) + ranked('../input/nesp-relaxed-rosetta-scores/submission_rosetta_scores')\n\n    \n    df.to_csv('ensemble_submission.csv', index=False)\n    !head ensemble_submission.csv    ","metadata":{"execution":{"iopub.status.busy":"2022-10-31T08:01:03.179782Z","iopub.execute_input":"2022-10-31T08:01:03.180205Z","iopub.status.idle":"2022-10-31T08:01:04.438494Z","shell.execute_reply.started":"2022-10-31T08:01:03.180167Z","shell.execute_reply":"2022-10-31T08:01:04.437114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf wandb ","metadata":{"execution":{"iopub.status.busy":"2022-10-31T07:57:53.340801Z","iopub.status.idle":"2022-10-31T07:57:53.341826Z","shell.execute_reply.started":"2022-10-31T07:57:53.341486Z","shell.execute_reply":"2022-10-31T07:57:53.341511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div class=\"alert alert-block alert-danger\" style=\"text-align:center; font-size:20px;\">\n    ‚ù§Ô∏è Dont forget to ‚ñ≤upvote‚ñ≤ if you find this notebook usefull!  ‚ù§Ô∏è\n</div>\n","metadata":{"execution":{"iopub.execute_input":"2022-10-11T17:06:17.775445Z","iopub.status.busy":"2022-10-11T17:06:17.775059Z","iopub.status.idle":"2022-10-11T17:06:17.783045Z","shell.execute_reply":"2022-10-11T17:06:17.781635Z","shell.execute_reply.started":"2022-10-11T17:06:17.775413Z"},"papermill":{"duration":1.385608,"end_time":"2022-10-13T08:13:48.869269","exception":false,"start_time":"2022-10-13T08:13:47.483661","status":"completed"},"tags":[]}}]}